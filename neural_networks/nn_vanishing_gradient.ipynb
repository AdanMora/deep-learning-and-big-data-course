{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semilla para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Red profunda con activaciones especificadas\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_size = 1\n",
    "        hidden_size = 128\n",
    "        depth = 20  # número de capas ocultas\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, 1))  # capa de salida\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Función para observar los gradientes de cada capa\n",
    "def get_gradients(model, x, loss_fn):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    output = model(x)\n",
    "    loss = loss_fn(output, torch.ones_like(output))\n",
    "    loss.backward()\n",
    "    grads = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            grads.append(param.grad.norm().item())  # norma del gradiente del peso\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear entradas y redes\n",
    "x = torch.randn(1, 1)\n",
    "\n",
    "model_sigmoid = DeepNetwork(nn.Sigmoid)\n",
    "model_relu = DeepNetwork(nn.ReLU)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Obtener gradientes\n",
    "grads_sigmoid = get_gradients(model_sigmoid, x, loss_fn)\n",
    "grads_relu = get_gradients(model_relu, x, loss_fn)\n",
    "\n",
    "# Graficar\n",
    "plt.plot(grads_sigmoid, label=\"Sigmoid\", marker=\"o\")\n",
    "plt.plot(grads_relu, label=\"ReLU\", marker=\"x\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Gradient norm (log scale)\")\n",
    "plt.title(\"Vanishing Gradient: Sigmoid vs ReLU\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Red profunda con opción de batch norm\n",
    "class DeepHighDimNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, activation_fn, use_batchnorm=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        hidden_size = 512\n",
    "        depth = 20\n",
    "\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            input_dim = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_gradients(model, x, loss_fn):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    output = model(x)\n",
    "    loss = loss_fn(output, torch.ones_like(output))\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            grads.append(param.grad.norm().item())\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Datos de entrada de alta dimensión\n",
    "input_dim = 512\n",
    "x = torch.randn(32, input_dim)\n",
    "\n",
    "# Modelos\n",
    "model_sigmoid = DeepHighDimNetwork(input_dim, nn.Sigmoid, use_batchnorm=False)\n",
    "model_sigmoid_bn = DeepHighDimNetwork(input_dim, nn.Sigmoid, use_batchnorm=True)\n",
    "model_relu = DeepHighDimNetwork(input_dim, nn.ReLU, use_batchnorm=False)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Obtener gradientes\n",
    "grads_sigmoid = get_gradients(model_sigmoid, x, loss_fn)\n",
    "grads_sigmoid_bn = get_gradients(model_sigmoid_bn, x, loss_fn)\n",
    "grads_relu = get_gradients(model_relu, x, loss_fn)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grads_sigmoid, label=\"Sigmoid (sin BatchNorm)\", marker=\"o\")\n",
    "plt.plot(grads_sigmoid_bn, label=\"Sigmoid + BatchNorm\", marker=\"^\")\n",
    "plt.plot(grads_relu, label=\"ReLU (referencia)\", marker=\"x\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Gradient norm (log scale)\")\n",
    "plt.title(\"Vanishing Gradient con y sin BatchNorm (dim=512)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
