{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='validation')\n",
    "train_texts = dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['', ' = Homarus gammarus = \\n', '', ' Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n', ''])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "tokens = [tokenize(line) for line in train_texts if line.strip()]\n",
    "flat_tokens = list(chain.from_iterable(tokens))\n",
    "vocab_counter = Counter(flat_tokens)\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab_counter.items())}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17601"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: '=',\n",
       " 3: 'homarus',\n",
       " 4: 'gammarus',\n",
       " 5: ',',\n",
       " 6: 'known',\n",
       " 7: 'as',\n",
       " 8: 'the',\n",
       " 9: 'european',\n",
       " 10: 'lobster',\n",
       " 11: 'or',\n",
       " 12: 'common',\n",
       " 13: 'is',\n",
       " 14: 'a',\n",
       " 15: 'species',\n",
       " 16: 'of',\n",
       " 17: 'clawed',\n",
       " 18: 'from',\n",
       " 19: 'eastern',\n",
       " 20: 'atlantic',\n",
       " 21: 'ocean',\n",
       " 22: 'mediterranean',\n",
       " 23: 'sea',\n",
       " 24: 'and',\n",
       " 25: 'parts',\n",
       " 26: 'black',\n",
       " 27: '.',\n",
       " 28: 'it',\n",
       " 29: 'closely',\n",
       " 30: 'related',\n",
       " 31: 'to',\n",
       " 32: 'american',\n",
       " 33: 'h.',\n",
       " 34: 'americanus',\n",
       " 35: 'may',\n",
       " 36: 'grow',\n",
       " 37: 'length',\n",
       " 38: '60',\n",
       " 39: 'cm',\n",
       " 40: '(',\n",
       " 41: '24',\n",
       " 42: 'in',\n",
       " 43: ')',\n",
       " 44: 'mass',\n",
       " 45: '6',\n",
       " 46: 'kilograms',\n",
       " 47: '13',\n",
       " 48: 'lb',\n",
       " 49: 'bears',\n",
       " 50: 'conspicuous',\n",
       " 51: 'pair',\n",
       " 52: 'claws',\n",
       " 53: 'life',\n",
       " 54: 'lobsters',\n",
       " 55: 'are',\n",
       " 56: 'blue',\n",
       " 57: 'only',\n",
       " 58: 'becoming',\n",
       " 59: '\"',\n",
       " 60: 'red',\n",
       " 61: 'on',\n",
       " 62: 'cooking',\n",
       " 63: 'mating',\n",
       " 64: 'occurs',\n",
       " 65: 'summer',\n",
       " 66: 'producing',\n",
       " 67: 'eggs',\n",
       " 68: 'which',\n",
       " 69: 'carried',\n",
       " 70: 'by',\n",
       " 71: 'females',\n",
       " 72: 'for',\n",
       " 73: 'up',\n",
       " 74: 'year',\n",
       " 75: 'before',\n",
       " 76: 'hatching',\n",
       " 77: 'into',\n",
       " 78: 'planktonic',\n",
       " 79: 'larvae',\n",
       " 80: 'highly',\n",
       " 81: 'esteemed',\n",
       " 82: 'food',\n",
       " 83: 'widely',\n",
       " 84: 'caught',\n",
       " 85: 'using',\n",
       " 86: 'pots',\n",
       " 87: 'mostly',\n",
       " 88: 'around',\n",
       " 89: 'british',\n",
       " 90: 'isles',\n",
       " 91: 'description',\n",
       " 92: 'large',\n",
       " 93: 'crustacean',\n",
       " 94: 'with',\n",
       " 95: 'body',\n",
       " 96: 'centimetres',\n",
       " 97: 'weighing',\n",
       " 98: '5',\n",
       " 99: '–',\n",
       " 100: '11',\n",
       " 101: 'although',\n",
       " 102: 'usually',\n",
       " 103: '23',\n",
       " 104: '38',\n",
       " 105: '9',\n",
       " 106: '15',\n",
       " 107: 'long',\n",
       " 108: 'weigh',\n",
       " 109: '0',\n",
       " 110: '@.@',\n",
       " 111: '7',\n",
       " 112: '2',\n",
       " 113: 'kg',\n",
       " 114: '1',\n",
       " 115: '4',\n",
       " 116: 'like',\n",
       " 117: 'other',\n",
       " 118: 'crustaceans',\n",
       " 119: 'have',\n",
       " 120: 'hard',\n",
       " 121: 'exoskeleton',\n",
       " 122: 'they',\n",
       " 123: 'must',\n",
       " 124: 'shed',\n",
       " 125: 'order',\n",
       " 126: 'process',\n",
       " 127: 'called',\n",
       " 128: 'ecdysis',\n",
       " 129: 'moulting',\n",
       " 130: 'this',\n",
       " 131: 'occur',\n",
       " 132: 'several',\n",
       " 133: 'times',\n",
       " 134: 'young',\n",
       " 135: 'but',\n",
       " 136: 'decreases',\n",
       " 137: 'once',\n",
       " 138: 'every',\n",
       " 139: 'years',\n",
       " 140: 'larger',\n",
       " 141: 'animals',\n",
       " 142: 'first',\n",
       " 143: 'pereiopods',\n",
       " 144: 'armed',\n",
       " 145: 'asymmetrical',\n",
       " 146: 'one',\n",
       " 147: 'crusher',\n",
       " 148: 'has',\n",
       " 149: 'rounded',\n",
       " 150: 'nodules',\n",
       " 151: 'used',\n",
       " 152: 'crushing',\n",
       " 153: 'prey',\n",
       " 154: ';',\n",
       " 155: 'cutter',\n",
       " 156: 'sharp',\n",
       " 157: 'inner',\n",
       " 158: 'edges',\n",
       " 159: 'holding',\n",
       " 160: 'tearing',\n",
       " 161: 'left',\n",
       " 162: 'claw',\n",
       " 163: 'right',\n",
       " 164: 'generally',\n",
       " 165: 'above',\n",
       " 166: 'spots',\n",
       " 167: 'that',\n",
       " 168: 'coalesce',\n",
       " 169: 'yellow',\n",
       " 170: 'below',\n",
       " 171: 'colour',\n",
       " 172: 'associated',\n",
       " 173: 'appears',\n",
       " 174: 'after',\n",
       " 175: 'because',\n",
       " 176: 'pigment',\n",
       " 177: 'astaxanthin',\n",
       " 178: 'bound',\n",
       " 179: 'protein',\n",
       " 180: 'complex',\n",
       " 181: 'broken',\n",
       " 182: 'heat',\n",
       " 183: 'releasing',\n",
       " 184: 'closest',\n",
       " 185: 'relative',\n",
       " 186: 'two',\n",
       " 187: 'very',\n",
       " 188: 'similar',\n",
       " 189: 'can',\n",
       " 190: 'be',\n",
       " 191: 'crossed',\n",
       " 192: 'artificially',\n",
       " 193: 'hybrids',\n",
       " 194: 'unlikely',\n",
       " 195: 'wild',\n",
       " 196: 'since',\n",
       " 197: 'their',\n",
       " 198: 'ranges',\n",
       " 199: 'do',\n",
       " 200: 'not',\n",
       " 201: 'overlap',\n",
       " 202: 'distinguished',\n",
       " 203: 'number',\n",
       " 204: 'characteristics',\n",
       " 205: ':',\n",
       " 206: 'rostrum',\n",
       " 207: 'more',\n",
       " 208: 'spines',\n",
       " 209: 'underside',\n",
       " 210: 'lacking',\n",
       " 211: '@-@',\n",
       " 212: 'tipped',\n",
       " 213: 'while',\n",
       " 214: 'those',\n",
       " 215: 'white',\n",
       " 216: 'orange',\n",
       " 217: 'creamy',\n",
       " 218: 'pale',\n",
       " 219: 'cycle',\n",
       " 220: 'female',\n",
       " 221: 'reach',\n",
       " 222: 'sexual',\n",
       " 223: 'maturity',\n",
       " 224: 'when',\n",
       " 225: 'grown',\n",
       " 226: 'carapace',\n",
       " 227: '80',\n",
       " 228: '85',\n",
       " 229: 'millimetres',\n",
       " 230: '3',\n",
       " 231: 'whereas',\n",
       " 232: 'males',\n",
       " 233: 'mature',\n",
       " 234: 'at',\n",
       " 235: 'slightly',\n",
       " 236: 'smaller',\n",
       " 237: 'size',\n",
       " 238: 'typically',\n",
       " 239: 'between',\n",
       " 240: 'recently',\n",
       " 241: 'moulted',\n",
       " 242: 'whose',\n",
       " 243: 'shell',\n",
       " 244: 'therefore',\n",
       " 245: 'soft',\n",
       " 246: 'shelled',\n",
       " 247: 'male',\n",
       " 248: 'carries',\n",
       " 249: '12',\n",
       " 250: 'months',\n",
       " 251: 'depending',\n",
       " 252: 'temperature',\n",
       " 253: 'attached',\n",
       " 254: 'her',\n",
       " 255: 'pleopods',\n",
       " 256: 'carrying',\n",
       " 257: 'said',\n",
       " 258: 'berried',\n",
       " 259: 'found',\n",
       " 260: 'throughout',\n",
       " 261: 'hatch',\n",
       " 262: 'night',\n",
       " 263: 'swim',\n",
       " 264: 'water',\n",
       " 265: 'surface',\n",
       " 266: 'where',\n",
       " 267: 'drift',\n",
       " 268: 'currents',\n",
       " 269: 'preying',\n",
       " 270: 'zooplankton',\n",
       " 271: 'stage',\n",
       " 272: 'involves',\n",
       " 273: 'three',\n",
       " 274: 'moults',\n",
       " 275: 'lasts',\n",
       " 276: '35',\n",
       " 277: 'days',\n",
       " 278: 'third',\n",
       " 279: 'moult',\n",
       " 280: 'juvenile',\n",
       " 281: 'takes',\n",
       " 282: 'form',\n",
       " 283: 'closer',\n",
       " 284: 'adult',\n",
       " 285: 'adopts',\n",
       " 286: 'benthic',\n",
       " 287: 'lifestyle',\n",
       " 288: 'juveniles',\n",
       " 289: 'rarely',\n",
       " 290: 'seen',\n",
       " 291: 'poorly',\n",
       " 292: 'capable',\n",
       " 293: 'digging',\n",
       " 294: 'extensive',\n",
       " 295: 'burrows',\n",
       " 296: 'estimated',\n",
       " 297: 'larva',\n",
       " 298: '20',\n",
       " 299: '@,@',\n",
       " 300: '000',\n",
       " 301: 'survives',\n",
       " 302: 'phase',\n",
       " 303: 'mm',\n",
       " 304: '59',\n",
       " 305: 'leave',\n",
       " 306: 'start',\n",
       " 307: 'lives',\n",
       " 308: 'distribution',\n",
       " 309: 'across',\n",
       " 310: 'north',\n",
       " 311: 'northern',\n",
       " 312: 'norway',\n",
       " 313: 'azores',\n",
       " 314: 'morocco',\n",
       " 315: 'including',\n",
       " 316: 'baltic',\n",
       " 317: 'also',\n",
       " 318: 'present',\n",
       " 319: 'most',\n",
       " 320: 'missing',\n",
       " 321: 'section',\n",
       " 322: 'east',\n",
       " 323: 'crete',\n",
       " 324: 'along',\n",
       " 325: 'west',\n",
       " 326: 'coast',\n",
       " 327: 'northernmost',\n",
       " 328: 'populations',\n",
       " 329: 'norwegian',\n",
       " 330: 'fjords',\n",
       " 331: 'tysfjorden',\n",
       " 332: 'nordfolda',\n",
       " 333: 'inside',\n",
       " 334: 'arctic',\n",
       " 335: 'circle',\n",
       " 336: 'divided',\n",
       " 337: 'four',\n",
       " 338: 'genetically',\n",
       " 339: 'distinct',\n",
       " 340: 'widespread',\n",
       " 341: 'population',\n",
       " 342: 'diverged',\n",
       " 343: 'due',\n",
       " 344: 'small',\n",
       " 345: 'effective',\n",
       " 346: 'sizes',\n",
       " 347: 'possibly',\n",
       " 348: 'adaptation',\n",
       " 349: 'local',\n",
       " 350: 'environment',\n",
       " 351: 'these',\n",
       " 352: 'been',\n",
       " 353: 'referred',\n",
       " 354: 'midnight',\n",
       " 355: 'sun',\n",
       " 356: 'last',\n",
       " 357: 'netherlands',\n",
       " 358: 'samples',\n",
       " 359: 'oosterschelde',\n",
       " 360: 'were',\n",
       " 361: 'collected',\n",
       " 362: 'english',\n",
       " 363: 'channel',\n",
       " 364: 'attempts',\n",
       " 365: 'made',\n",
       " 366: 'introduce',\n",
       " 367: 'new',\n",
       " 368: 'zealand',\n",
       " 369: 'alongside',\n",
       " 370: 'such',\n",
       " 371: 'edible',\n",
       " 372: 'crab',\n",
       " 373: 'cancer',\n",
       " 374: 'pagurus',\n",
       " 375: '1904',\n",
       " 376: '1914',\n",
       " 377: 'million',\n",
       " 378: 'released',\n",
       " 379: 'hatcheries',\n",
       " 380: 'dunedin',\n",
       " 381: 'did',\n",
       " 382: 'become',\n",
       " 383: 'established',\n",
       " 384: 'there',\n",
       " 385: 'ecology',\n",
       " 386: 'live',\n",
       " 387: 'continental',\n",
       " 388: 'shelf',\n",
       " 389: 'depths',\n",
       " 390: '150',\n",
       " 391: 'metres',\n",
       " 392: '492',\n",
       " 393: 'ft',\n",
       " 394: 'normally',\n",
       " 395: 'deeper',\n",
       " 396: 'than',\n",
       " 397: '50',\n",
       " 398: 'm',\n",
       " 399: '160',\n",
       " 400: 'prefer',\n",
       " 401: 'substrates',\n",
       " 402: 'rocks',\n",
       " 403: 'mud',\n",
       " 404: 'holes',\n",
       " 405: 'crevices',\n",
       " 406: 'emerging',\n",
       " 407: 'feed',\n",
       " 408: 'diet',\n",
       " 409: 'consists',\n",
       " 410: 'invertebrates',\n",
       " 411: 'include',\n",
       " 412: 'crabs',\n",
       " 413: 'molluscs',\n",
       " 414: 'urchins',\n",
       " 415: 'starfish',\n",
       " 416: 'polychaete',\n",
       " 417: 'worms',\n",
       " 418: 'nephrops',\n",
       " 419: 'norvegicus',\n",
       " 420: 'hosts',\n",
       " 421: 'animal',\n",
       " 422: 'phylum',\n",
       " 423: 'cycliophora',\n",
       " 424: 'described',\n",
       " 425: 'susceptible',\n",
       " 426: 'disease',\n",
       " 427: 'gaffkaemia',\n",
       " 428: 'caused',\n",
       " 429: 'bacterium',\n",
       " 430: 'aerococcus',\n",
       " 431: 'viridans',\n",
       " 432: 'frequently',\n",
       " 433: 'captive',\n",
       " 434: 'prior',\n",
       " 435: 'occupation',\n",
       " 436: 'tanks',\n",
       " 437: 'could',\n",
       " 438: 'ruled',\n",
       " 439: 'out',\n",
       " 440: 'human',\n",
       " 441: 'consumption',\n",
       " 442: 'traditionally',\n",
       " 443: 'foodstuff',\n",
       " 444: 'was',\n",
       " 445: 'mentioned',\n",
       " 446: 'crabfish',\n",
       " 447: 'seventeenth',\n",
       " 448: 'century',\n",
       " 449: 'folk',\n",
       " 450: 'song',\n",
       " 451: 'fetch',\n",
       " 452: 'high',\n",
       " 453: 'prices',\n",
       " 454: 'sold',\n",
       " 455: 'fresh',\n",
       " 456: 'frozen',\n",
       " 457: 'canned',\n",
       " 458: 'powdered',\n",
       " 459: 'both',\n",
       " 460: 'abdomen',\n",
       " 461: 'contain',\n",
       " 462: 'excellent',\n",
       " 463: 'meat',\n",
       " 464: 'contents',\n",
       " 465: 'cephalothorax',\n",
       " 466: 'exceptions',\n",
       " 467: 'gastric',\n",
       " 468: 'mill',\n",
       " 469: 'sand',\n",
       " 470: 'vein',\n",
       " 471: 'gut',\n",
       " 472: 'price',\n",
       " 473: 'higher',\n",
       " 474: 'considered',\n",
       " 475: 'better',\n",
       " 476: 'flavour',\n",
       " 477: 'fished',\n",
       " 478: 'lines',\n",
       " 479: 'baited',\n",
       " 480: 'octopus',\n",
       " 481: 'cuttlefish',\n",
       " 482: 'sometimes',\n",
       " 483: 'succeed',\n",
       " 484: 'tempting',\n",
       " 485: 'them',\n",
       " 486: 'allow',\n",
       " 487: 'net',\n",
       " 488: 'hand',\n",
       " 489: '2008',\n",
       " 490: '386',\n",
       " 491: 't',\n",
       " 492: 'europe',\n",
       " 493: 'africa',\n",
       " 494: '462',\n",
       " 495: '79',\n",
       " 496: '%',\n",
       " 497: 'islands',\n",
       " 498: 'minimum',\n",
       " 499: 'landing',\n",
       " 500: '87',\n",
       " 501: 'aquaculture',\n",
       " 502: 'systems',\n",
       " 503: 'under',\n",
       " 504: 'development',\n",
       " 505: 'production',\n",
       " 506: 'rates',\n",
       " 507: 'still',\n",
       " 508: 'low',\n",
       " 509: 'taxonomic',\n",
       " 510: 'history',\n",
       " 511: 'given',\n",
       " 512: 'binomial',\n",
       " 513: 'name',\n",
       " 514: 'carl',\n",
       " 515: 'linnaeus',\n",
       " 516: 'tenth',\n",
       " 517: 'edition',\n",
       " 518: 'his',\n",
       " 519: 'systema',\n",
       " 520: 'naturae',\n",
       " 521: 'published',\n",
       " 522: '1758',\n",
       " 523: \"'\",\n",
       " 524: 'concept',\n",
       " 525: 'genus',\n",
       " 526: 'time',\n",
       " 527: 'included',\n",
       " 528: 'all',\n",
       " 529: 'type',\n",
       " 530: 'weber',\n",
       " 531: '1795',\n",
       " 532: 'determined',\n",
       " 533: 'direction',\n",
       " 534: '51',\n",
       " 535: 'international',\n",
       " 536: 'commission',\n",
       " 537: 'zoological',\n",
       " 538: 'nomenclature',\n",
       " 539: 'confusion',\n",
       " 540: 'arose',\n",
       " 541: 'had',\n",
       " 542: 'different',\n",
       " 543: 'names',\n",
       " 544: 'astacus',\n",
       " 545: 'marinus',\n",
       " 546: 'fabricius',\n",
       " 547: '1775',\n",
       " 548: 'vulgaris',\n",
       " 549: 'milne',\n",
       " 550: 'edwards',\n",
       " 551: '1837',\n",
       " 552: 'friedrich',\n",
       " 553: \"'s\",\n",
       " 554: 'overlooked',\n",
       " 555: 'until',\n",
       " 556: 'rediscovered',\n",
       " 557: 'mary',\n",
       " 558: 'rathbun',\n",
       " 559: 'rendering',\n",
       " 560: 'any',\n",
       " 561: 'assignments',\n",
       " 562: 'invalid',\n",
       " 563: 'specimen',\n",
       " 564: 'lectotype',\n",
       " 565: 'selected',\n",
       " 566: 'lipke',\n",
       " 567: 'holthuis',\n",
       " 568: '1974',\n",
       " 569: 'came',\n",
       " 570: '57',\n",
       " 571: '°',\n",
       " 572: '53',\n",
       " 573: '′',\n",
       " 574: 'n',\n",
       " 575: '32',\n",
       " 576: 'e',\n",
       " 577: 'near',\n",
       " 578: 'marstrand',\n",
       " 579: 'sweden',\n",
       " 580: '48',\n",
       " 581: 'kilometres',\n",
       " 582: '30',\n",
       " 583: 'miles',\n",
       " 584: 'northwest',\n",
       " 585: 'gothenburg',\n",
       " 586: 'paralectotypes',\n",
       " 587: 'lost',\n",
       " 588: 'preferred',\n",
       " 589: 'agriculture',\n",
       " 590: 'organization',\n",
       " 591: 'frank',\n",
       " 592: 'headlam',\n",
       " 593: 'air',\n",
       " 594: 'vice',\n",
       " 595: 'marshal',\n",
       " 596: 'cb',\n",
       " 597: 'cbe',\n",
       " 598: 'july',\n",
       " 599: 'december',\n",
       " 600: '1976',\n",
       " 601: 'senior',\n",
       " 602: 'commander',\n",
       " 603: 'royal',\n",
       " 604: 'australian',\n",
       " 605: 'force',\n",
       " 606: 'raaf',\n",
       " 607: 'born',\n",
       " 608: 'educated',\n",
       " 609: 'tasmania',\n",
       " 610: 'he',\n",
       " 611: 'joined',\n",
       " 612: 'an',\n",
       " 613: 'cadet',\n",
       " 614: 'january',\n",
       " 615: '1934',\n",
       " 616: 'specialised',\n",
       " 617: 'flying',\n",
       " 618: 'instruction',\n",
       " 619: 'navigation',\n",
       " 620: 'outbreak',\n",
       " 621: 'world',\n",
       " 622: 'war',\n",
       " 623: 'ii',\n",
       " 624: 'april',\n",
       " 625: '1941',\n",
       " 626: 'became',\n",
       " 627: 'commanding',\n",
       " 628: 'officer',\n",
       " 629: 'no.',\n",
       " 630: 'squadron',\n",
       " 631: 'operated',\n",
       " 632: 'lockheed',\n",
       " 633: 'hudsons',\n",
       " 634: 'deployed',\n",
       " 635: 'dutch',\n",
       " 636: 'timor',\n",
       " 637: 'saw',\n",
       " 638: 'action',\n",
       " 639: 'against',\n",
       " 640: 'japanese',\n",
       " 641: 'forces',\n",
       " 642: 'south',\n",
       " 643: 'pacific',\n",
       " 644: 'returning',\n",
       " 645: 'australia',\n",
       " 646: 'february',\n",
       " 647: '1942',\n",
       " 648: 'held',\n",
       " 649: 'staff',\n",
       " 650: 'appointments',\n",
       " 651: 'training',\n",
       " 652: 'commands',\n",
       " 653: 'finishing',\n",
       " 654: 'group',\n",
       " 655: 'captain',\n",
       " 656: 'served',\n",
       " 657: 'western',\n",
       " 658: 'area',\n",
       " 659: '1946',\n",
       " 660: 'director',\n",
       " 661: '1947',\n",
       " 662: '1950',\n",
       " 663: 'during',\n",
       " 664: 'malayan',\n",
       " 665: 'emergency',\n",
       " 666: 'stationed',\n",
       " 667: 'singapore',\n",
       " 668: '90',\n",
       " 669: 'composite',\n",
       " 670: 'wing',\n",
       " 671: 'later',\n",
       " 672: 'raf',\n",
       " 673: 'tengah',\n",
       " 674: 'twice',\n",
       " 675: 'acting',\n",
       " 676: 'member',\n",
       " 677: 'personnel',\n",
       " 678: '1957',\n",
       " 679: '1959',\n",
       " 680: 'receiving',\n",
       " 681: 'appointment',\n",
       " 682: 'empire',\n",
       " 683: '1958',\n",
       " 684: 'promoted',\n",
       " 685: 'successively',\n",
       " 686: 'positions',\n",
       " 687: 'aoc',\n",
       " 688: 'operational',\n",
       " 689: 'command',\n",
       " 690: '1961',\n",
       " 691: '62',\n",
       " 692: '224',\n",
       " 693: '1962',\n",
       " 694: '1965',\n",
       " 695: 'indonesia',\n",
       " 696: 'malaysia',\n",
       " 697: 'konfrontasi',\n",
       " 698: 'deputy',\n",
       " 699: 'chief',\n",
       " 700: '66',\n",
       " 701: 'support',\n",
       " 702: '1966',\n",
       " 703: '67',\n",
       " 704: 'appointed',\n",
       " 705: 'companion',\n",
       " 706: 'bath',\n",
       " 707: 'following',\n",
       " 708: 'posting',\n",
       " 709: 'london',\n",
       " 710: 'head',\n",
       " 711: 'joint',\n",
       " 712: 'services',\n",
       " 713: '1968',\n",
       " 714: '1971',\n",
       " 715: 'retired',\n",
       " 716: 'died',\n",
       " 717: 'melbourne',\n",
       " 718: 'five',\n",
       " 719: 'early',\n",
       " 720: 'career',\n",
       " 721: 'son',\n",
       " 722: 'farmers',\n",
       " 723: 'malcolm',\n",
       " 724: 'hilda',\n",
       " 725: 'launceston',\n",
       " 726: 'schooled',\n",
       " 727: 'clemes',\n",
       " 728: 'college',\n",
       " 729: 'hobart',\n",
       " 730: 'matriculated',\n",
       " 731: '1932',\n",
       " 732: 'wishes',\n",
       " 733: 'parents',\n",
       " 734: '16',\n",
       " 735: 'underwent',\n",
       " 736: 'school',\n",
       " 737: 'fts',\n",
       " 738: 'point',\n",
       " 739: 'cook',\n",
       " 740: 'victoria',\n",
       " 741: 'commissioned',\n",
       " 742: 'pilot',\n",
       " 743: '1935',\n",
       " 744: 'completing',\n",
       " 745: 'conversion',\n",
       " 746: 'course',\n",
       " 747: 'assigned',\n",
       " 748: 'seaplane',\n",
       " 749: 'no',\n",
       " 750: 'flight',\n",
       " 751: 'according',\n",
       " 752: 'official',\n",
       " 753: 'pre',\n",
       " 754: 'part',\n",
       " 755: 'supermarine',\n",
       " 756: 'southampton',\n",
       " 757: 'boats',\n",
       " 758: 'de',\n",
       " 759: 'havilland',\n",
       " 760: 'gipsy',\n",
       " 761: 'moth',\n",
       " 762: 'floatplanes',\n",
       " 763: 'among',\n",
       " 764: 'types',\n",
       " 765: 'wrote',\n",
       " 766: 'paper',\n",
       " 767: 'national',\n",
       " 768: 'defence',\n",
       " 769: 'suggested',\n",
       " 770: 'strong',\n",
       " 771: 'naval',\n",
       " 772: 'submarines',\n",
       " 773: 'fixed',\n",
       " 774: 'defences',\n",
       " 775: 'practically',\n",
       " 776: 'invulnerable',\n",
       " 777: 'historian',\n",
       " 778: 'alan',\n",
       " 779: 'stephens',\n",
       " 780: 'effect',\n",
       " 781: 'defined',\n",
       " 782: 'anti',\n",
       " 783: 'lodgment',\n",
       " 784: 'persistent',\n",
       " 785: 'feature',\n",
       " 786: 'strategic',\n",
       " 787: 'thinking',\n",
       " 788: 'completed',\n",
       " 789: 'instructors',\n",
       " 790: '1936',\n",
       " 791: 'lieutenant',\n",
       " 792: 'march',\n",
       " 793: '1937',\n",
       " 794: 'commencing',\n",
       " 795: '1938',\n",
       " 796: 'six',\n",
       " 797: 'students',\n",
       " 798: 'take',\n",
       " 799: 'specialist',\n",
       " 800: 'run',\n",
       " 801: 'lieutenants',\n",
       " 802: 'bill',\n",
       " 803: 'garing',\n",
       " 804: 'alister',\n",
       " 805: 'murdoch',\n",
       " 806: 'involved',\n",
       " 807: 'epic',\n",
       " 808: 'flights',\n",
       " 809: 'attracted',\n",
       " 810: 'considerable',\n",
       " 811: 'media',\n",
       " 812: 'attention',\n",
       " 813: 'twelve',\n",
       " 814: 'day',\n",
       " 815: '10',\n",
       " 816: '800',\n",
       " 817: 'kilometre',\n",
       " 818: '700',\n",
       " 819: 'mi',\n",
       " 820: 'round',\n",
       " 821: 'trip',\n",
       " 822: 'avro',\n",
       " 823: 'ansons',\n",
       " 824: 'piloted',\n",
       " 825: 'november',\n",
       " 826: 'month',\n",
       " 827: 'led',\n",
       " 828: 'journey',\n",
       " 829: 'back',\n",
       " 830: 'forth',\n",
       " 831: 'over',\n",
       " 832: 'central',\n",
       " 833: 'subsequently',\n",
       " 834: 'passed',\n",
       " 835: 'special',\n",
       " 836: 'distinction',\n",
       " 837: '27',\n",
       " 838: '1939',\n",
       " 839: 'posted',\n",
       " 840: 'station',\n",
       " 841: 'laverton',\n",
       " 842: 'initially',\n",
       " 843: 'transferring',\n",
       " 844: '29',\n",
       " 845: 'august',\n",
       " 846: 'units',\n",
       " 847: 'engaged',\n",
       " 848: 'convoy',\n",
       " 849: 'escort',\n",
       " 850: 'maritime',\n",
       " 851: 'reconnaissance',\n",
       " 852: 'duties',\n",
       " 853: 'off',\n",
       " 854: 'continued',\n",
       " 855: 'serve',\n",
       " 856: '1940',\n",
       " 857: 'headquarters',\n",
       " 858: 'leader',\n",
       " 859: 'june',\n",
       " 860: 'weeks',\n",
       " 861: 'married',\n",
       " 862: 'katherine',\n",
       " 863: 'bridge',\n",
       " 864: 'st',\n",
       " 865: 'paul',\n",
       " 866: 'anglican',\n",
       " 867: 'church',\n",
       " 868: 'frankston',\n",
       " 869: 'couple',\n",
       " 870: 'would',\n",
       " 871: 'daughter',\n",
       " 872: 'raised',\n",
       " 873: 'equipped',\n",
       " 874: 'mainly',\n",
       " 875: 'conducted',\n",
       " 876: 'patrols',\n",
       " 877: 'southern',\n",
       " 878: 'waters',\n",
       " 879: 'its',\n",
       " 880: 'aircraft',\n",
       " 881: 'ordered',\n",
       " 882: 'darwin',\n",
       " 883: 'territory',\n",
       " 884: 'response',\n",
       " 885: 'fears',\n",
       " 886: 'aggression',\n",
       " 887: 'detachment',\n",
       " 888: 'itself',\n",
       " 889: 'penfui',\n",
       " 890: 'koepang',\n",
       " 891: 'eight',\n",
       " 892: 'remaining',\n",
       " 893: 'standby',\n",
       " 894: 'aware',\n",
       " 895: 'now',\n",
       " 896: 'based',\n",
       " 897: 'attacked',\n",
       " 898: 'pearler',\n",
       " 899: 'nanyo',\n",
       " 900: 'maru',\n",
       " 901: 'suspected',\n",
       " 902: 'being',\n",
       " 903: 'radio',\n",
       " 904: 'ship',\n",
       " 905: 'forced',\n",
       " 906: 'aground',\n",
       " 907: 'transferred',\n",
       " 908: 'base',\n",
       " 909: 'well',\n",
       " 910: 'dispersed',\n",
       " 911: 'boeroe',\n",
       " 912: 'island',\n",
       " 913: 'shipping',\n",
       " 914: 'taking',\n",
       " 915: 'invasion',\n",
       " 916: 'celebes',\n",
       " 917: 'shot',\n",
       " 918: 'down',\n",
       " 919: 'damaged',\n",
       " 920: 'bombing',\n",
       " 921: 'transport',\n",
       " 922: 'next',\n",
       " 923: 'themselves',\n",
       " 924: 'mitsubishi',\n",
       " 925: 'zeros',\n",
       " 926: 'bombed',\n",
       " 927: '26',\n",
       " 928: 'regularly',\n",
       " 929: 'thereafter',\n",
       " 930: 'damaging',\n",
       " 931: 'some',\n",
       " 932: 'intact',\n",
       " 933: 'withdrawn',\n",
       " 934: 'remained',\n",
       " 935: 'enable',\n",
       " 936: 'missions',\n",
       " 937: '18',\n",
       " 938: 'evacuate',\n",
       " 939: 'except',\n",
       " 940: 'party',\n",
       " 941: 'demolish',\n",
       " 942: 'airfield',\n",
       " 943: 'assistance',\n",
       " 944: 'sparrow',\n",
       " 945: 'returned',\n",
       " 946: 'just',\n",
       " 947: 'city',\n",
       " 948: 'experienced',\n",
       " 949: 'raid',\n",
       " 950: 'destroyed',\n",
       " 951: 'attack',\n",
       " 952: 'remainder',\n",
       " 953: 'relocated',\n",
       " 954: 'daly',\n",
       " 955: 'carry',\n",
       " 956: 'targets',\n",
       " 957: 'controller',\n",
       " 958: 'operations',\n",
       " 959: 'nhill',\n",
       " 960: 'operating',\n",
       " 961: '97',\n",
       " 962: 'reserve',\n",
       " 963: 'formed',\n",
       " 964: '1943',\n",
       " 965: 'took',\n",
       " 966: 'observer',\n",
       " 967: 'aos',\n",
       " 968: 'mount',\n",
       " 969: 'gambier',\n",
       " 970: 'inaugural',\n",
       " 971: 'fairey',\n",
       " 972: 'battles',\n",
       " 973: 'port',\n",
       " 974: 'pirie',\n",
       " 975: 'handing',\n",
       " 976: 'commenced',\n",
       " 977: 'studies',\n",
       " 978: 'martha',\n",
       " 979: 'october',\n",
       " 980: '1944',\n",
       " 981: 'administrative',\n",
       " 982: '1945',\n",
       " 983: 'post',\n",
       " 984: 'britain',\n",
       " 985: 'end',\n",
       " 986: 'attended',\n",
       " 987: 'andover',\n",
       " 988: 'overseas',\n",
       " 989: 'return',\n",
       " 990: 'paddy',\n",
       " 991: 'heffernan',\n",
       " 992: 'headquartered',\n",
       " 993: 'changi',\n",
       " 994: 'controlled',\n",
       " 995: 'bomber',\n",
       " 996: 'lincolns',\n",
       " 997: 'douglas',\n",
       " 998: 'c',\n",
       " 999: '47',\n",
       " 1000: 'dakotas',\n",
       " 1001: 'communist',\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, lines, seq_len):\n",
    "        self.data = []\n",
    "        for line in lines:\n",
    "            tokens = tokenize(line)\n",
    "            ids = encode(tokens)\n",
    "            for i in range(len(ids) - seq_len):\n",
    "                input_seq = ids[i:i+seq_len]\n",
    "                target = ids[i+1:i+seq_len+1]\n",
    "                self.data.append((torch.tensor(input_seq), torch.tensor(target)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = LanguageModelDataset(train_texts, sequence_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "secuencia = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secuencia[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secuencia[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13097,    27,    28,   148,  1248,    42, 12853,  4546,    40,  6819,\n",
       "           43,    72,     8, 12856,     5, 12853,  3467,    40,  2254,    43,\n",
       "           72,     8, 12855, 13098,    24, 13099,     5, 12853, 13081,  2872])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secuencia[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   27,    28,   148,  1248,    42, 12853,  4546,    40,  6819,    43,\n",
       "           72,     8, 12856,     5, 12853,  3467,    40,  2254,    43,    72,\n",
       "            8, 12855, 13098,    24, 13099,     5, 12853, 13081,  2872,    40])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secuencia[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computers',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'appeared',\n",
       " 'in',\n",
       " 'sonic',\n",
       " 'compilation',\n",
       " '(',\n",
       " '1995',\n",
       " ')',\n",
       " 'for',\n",
       " 'the',\n",
       " 'genesis',\n",
       " ',',\n",
       " 'sonic',\n",
       " 'jam',\n",
       " '(',\n",
       " '1997',\n",
       " ')',\n",
       " 'for',\n",
       " 'the',\n",
       " 'sega',\n",
       " 'saturn',\n",
       " 'and',\n",
       " 'game.com',\n",
       " ',',\n",
       " 'sonic',\n",
       " 'mega',\n",
       " 'collection']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = secuencia[0][0].numpy().tolist()\n",
    "[inv_vocab[i] for i in s if i in inv_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computers . it has appeared in sonic compilation ( 1995 ) for the genesis , sonic jam ( 1997 ) for the sega saturn and game.com , sonic mega collection'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secuencia_str = ' '.join([inv_vocab[i] for i in s if i in inv_vocab])\n",
    "secuencia_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['computers', '.', 'it', 'has', 'appeared', 'in', 'sonic', 'compilation', '(', '1995', ')', 'for', 'the', 'genesis', ',', 'sonic', 'jam', '(', '1997', ')', 'for', 'the', 'sega', 'saturn', 'and', 'game', '.', 'com', ',', 'sonic', 'mega', 'collection']\n",
      "Token IDs: tensor([[  101,  7588,  1012,  2009,  2038,  2596,  1999, 12728,  6268,  1006,\n",
      "          2786,  1007,  2005,  1996, 11046,  1010, 12728,  9389,  1006,  2722,\n",
      "          1007,  2005,  1996, 16562, 14784,  1998,  2208,  1012,  4012,  1010,\n",
      "         12728, 13164,  3074,   102]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load pretrained tokenizer (BERT example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoding = tokenizer(secuencia_str, return_tensors=\"pt\")  # returns PyTorch tensors\n",
    "\n",
    "print(\"Tokenized text:\", tokenizer.tokenize(secuencia_str))\n",
    "print(\"Token IDs:\", encoding[\"input_ids\"])\n",
    "print(\"Attention mask:\", encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs Recurrent Neural Networks\n",
    "\n",
    "\n",
    "* Secuencias: series de tiempo, texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/rnn2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/rnns.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 10,  5, 13, 14, 15, 16, 17, 10,\n",
       "         18,  8, 19, 20, 21,  5, 22, 23, 24, 25, 16,  8]),\n",
       " tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 10,  5, 13, 14, 15, 16, 17, 10, 18,\n",
       "          8, 19, 20, 21,  5, 22, 23, 24, 25, 16,  8, 26]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 24200.61\n",
      "Epoch 2, Loss: 14645.70\n",
      "Epoch 3, Loss: 11250.27\n",
      "Epoch 4, Loss: 9600.56\n",
      "Epoch 5, Loss: 8647.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m output, hidden = model(inputs, hidden)\n\u001b[32m     13\u001b[39m loss = criterion(output.view(-\u001b[32m1\u001b[39m, vocab_size), targets.view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m optimizer.step()\n\u001b[32m     16\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qué pasa adentro de la RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "sequence = [x₁, x₂, x₃, ..., xₜ]  # xᵢ is a vector for each word\n",
    "\n",
    "h₀ = 0                       # initial hidden state (often zeros)\n",
    "h₁ = tanh(Wₓx₁ + Wₕh₀ + b)   ← step 1\n",
    "h₂ = tanh(Wₓx₂ + Wₕh₁ + b)   ← step 2\n",
    "h₃ = tanh(Wₓx₃ + Wₕh₂ + b)   ← step 3\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_word, max_len=20):\n",
    "    model.eval()\n",
    "    input = torch.tensor([[vocab.get(start_word, vocab[\"<unk>\"])]], dtype=torch.long).to(DEVICE)\n",
    "    hidden = model.init_hidden(1)\n",
    "    output_words = [start_word]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            output, hidden = model(input, hidden)\n",
    "            pred = output[:, -1, :]\n",
    "            word_id = torch.argmax(pred, dim=-1).item()\n",
    "            word = inv_vocab.get(word_id, \"<unk>\")\n",
    "            output_words.append(word)\n",
    "            input = torch.tensor([[word_id]], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial jane stork flew up supplies in the same period , wzks ( fm 104 @.@ 1 ) , wexr ( fm 106 @.@ 9 ) , wyhl ( am 1450 ) , and wkzb ( fm 95 @.@ 1 ) , wexr ( fm 106 @.@ 9 ) ,'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"artificial\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Long-Short Term Memory RNNs\n",
    "\n",
    "![Sample Image](./images/lstm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/lstm2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 26877.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m output = model(inputs)\n\u001b[32m     17\u001b[39m loss = criterion(output.view(-\u001b[32m1\u001b[39m, vocab_size), targets.view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/university/deep-learning-and-big-data-course/venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "epochs = 3\n",
    "\n",
    "model = LSTM(vocab_size, embed_size, hidden_size).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, \"mountain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Attention is all you need \n",
    "[link to paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/attention1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/attention2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Image](./images/attention3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5053,  0.9641, -0.9107,  0.8118,  0.5007,  1.7045,  0.9183,\n",
       "          -0.1551, -0.6815, -1.3406,  0.0037, -1.4043,  0.4271, -0.4164,\n",
       "           0.3124, -0.5856]],\n",
       "\n",
       "        [[-0.1164, -2.0877, -0.2474, -0.3168,  0.5530,  0.5337,  0.0444,\n",
       "           1.5327,  0.0402,  0.7689, -0.0411, -1.4002,  1.7931,  0.7889,\n",
       "           0.5393, -0.4003]],\n",
       "\n",
       "        [[ 2.2558, -2.2461, -0.7512, -0.9289,  0.8567,  0.0951, -1.2963,\n",
       "          -0.5458, -0.7677, -0.3184,  0.6283,  0.8285, -3.0088, -0.1173,\n",
       "          -1.2245, -1.1309]],\n",
       "\n",
       "        [[-0.1261, -0.6074,  1.1562,  0.1848,  0.1775, -0.7985,  1.2771,\n",
       "           0.6787, -0.3205,  1.0256,  2.1595, -1.8794,  0.7863, -0.5103,\n",
       "          -0.0580, -0.8438]],\n",
       "\n",
       "        [[-0.3784,  0.1903,  0.2804,  0.5082,  0.2474, -0.2217, -0.2444,\n",
       "          -0.2585, -0.7479,  1.0252,  0.1490,  1.8295, -2.5580, -0.0969,\n",
       "           0.2846,  0.6203]],\n",
       "\n",
       "        [[ 0.9551, -1.1598, -0.1502,  0.7282,  1.0783,  1.3903,  0.2799,\n",
       "           1.8911, -1.4785, -1.8323,  0.5093, -0.1038,  0.3939, -1.6327,\n",
       "          -0.6809,  1.6851]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Words for the example\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Dummy embeddings (simulate pretrained GloVe for simplicity)\n",
    "embedding_dim = 16\n",
    "vocab = {word: torch.randn(embedding_dim) for word in set(tokens)}\n",
    "embeddings = torch.stack([vocab[word] for word in tokens])  # (seq_len, embed_dim)\n",
    "\n",
    "# Reshape for MultiheadAttention: (seq_len, batch, embed_dim)\n",
    "x = embeddings.unsqueeze(1)  # (6, 1, 16)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FASTAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([6, 1, 16])\n",
      "Attention weights shape: torch.Size([1, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention layer\n",
    "mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=4, batch_first=False)\n",
    "\n",
    "# Apply self-attention\n",
    "output, attn_weights = mha(x, x, x)  # (seq_len, batch, embed_dim), (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1420, 0.1691, 0.2133, 0.1362, 0.1655, 0.1741],\n",
       "         [0.1342, 0.1708, 0.1522, 0.2886, 0.1288, 0.1253],\n",
       "         [0.1194, 0.0963, 0.1272, 0.1694, 0.2117, 0.2760],\n",
       "         [0.1177, 0.1310, 0.1757, 0.2207, 0.1906, 0.1643],\n",
       "         [0.1695, 0.1387, 0.1578, 0.1088, 0.2019, 0.2233],\n",
       "         [0.1228, 0.1717, 0.1579, 0.0860, 0.2186, 0.2431]]],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4963, 0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689, 0.2939, 0.5185],\n",
      "         [0.6977, 0.8000, 0.1610, 0.2823, 0.6816, 0.9152, 0.3971, 0.8742],\n",
      "         [0.4194, 0.5529, 0.9527, 0.0362, 0.1852, 0.3734, 0.3051, 0.9320]]])\n",
      "Attention Weights:\n",
      " tensor([[0.2474, 0.2407, 0.2605, 0.2515],\n",
      "        [0.2400, 0.2525, 0.2493, 0.2582],\n",
      "        [0.2485, 0.2424, 0.2651, 0.2441],\n",
      "        [0.2413, 0.2474, 0.2495, 0.2618]], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy input: (batch=1, seq_len=4, embed_dim=8)\n",
    "x = torch.rand(1, 4, 8)\n",
    "print(x)\n",
    "\n",
    "# Linear layers to get Q, K, V\n",
    "embed_dim = 8\n",
    "q_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "k_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "v_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "Q = q_proj(x)  # (1, 4, 8)\n",
    "K = k_proj(x)  # (1, 4, 8)\n",
    "V = v_proj(x)  # (1, 4, 8)\n",
    "\n",
    "# Attention scores\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (embed_dim ** 0.5)  # (1, 4, 4)\n",
    "\n",
    "# Softmax to get weights\n",
    "weights = F.softmax(scores, dim=-1)  # (1, 4, 4)\n",
    "\n",
    "# Final output\n",
    "output = torch.matmul(weights, V)  # (1, 4, 8)\n",
    "\n",
    "print(\"Attention Weights:\\n\", weights.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()], probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I absolutely love the new design of this app!\", \"The customer service was very disappointing.\", \"The weather is fine, nothing special. I hate it.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts, props  = predict_sentiment(texts=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Very Positive', 'Negative', 'Neutral']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0177, 0.0212, 0.0698, 0.2857, 0.6056],\n",
       "        [0.4311, 0.4863, 0.0444, 0.0185, 0.0197],\n",
       "        [0.0373, 0.0692, 0.8069, 0.0581, 0.0284]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
